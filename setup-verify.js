#!/usr/bin/env node

/**
 * SETUP VERIFICATION & INSTRUCTIONS
 * Shows status and next steps for Continue + MCP Tools
 */

const http = require('http');

async function checkOllama() {
  return new Promise((resolve) => {
    http.get('http://192.168.0.94:11434/api/tags', (res) => {
      let data = '';
      res.on('data', chunk => data += chunk);
      res.on('end', () => {
        try {
          const json = JSON.parse(data);
          resolve({ success: true, models: json.models || [] });
        } catch (e) {
          resolve({ success: false, error: e.message });
        }
      });
    }).on('error', (e) => {
      resolve({ success: false, error: e.message });
    });
  });
}

async function main() {
  console.log('\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
  console.log('â•‘       CONTINUE + MCP TOOLS - SETUP VERIFICATION               â•‘');
  console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

  // Check Continue config
  const fs = require('fs');
  const path = require('path');
  
  const continueConfig = path.join(process.env.USERPROFILE, '.continue', 'config.json');
  const configExists = fs.existsSync(continueConfig);
  
  console.log('ğŸ“‹ SETUP STATUS:\n');
  console.log(`  âœ“ MCP Server configured: YES`);
  console.log(`  âœ“ Continue config exists: ${configExists ? 'YES' : 'NO'}`);
  console.log(`  âœ“ MCP tools test files: YES`);
  
  // Check Ollama
  console.log('\nğŸ”Œ OLLAMA CONNECTION:\n');
  const ollama = await checkOllama();
  
  if (ollama.success) {
    console.log(`  âœ“ Connected to 192.168.0.94:11434`);
    console.log(`  âœ“ Models available: ${ollama.models.length}\n`);
    
    ollama.models.forEach(m => {
      const size = (m.size / 1024 / 1024 / 1024).toFixed(1);
      console.log(`     â€¢ ${m.name.padEnd(35)} ${size}GB`);
    });
  } else {
    console.log(`  âœ— Could not connect`);
    console.log(`  Error: ${ollama.error}\n`);
  }

  console.log('\nâ•'.repeat(66));
  console.log('ğŸ“¥ NEXT STEPS - PULL NEW MODELS:\n');
  
  console.log('The automated pull failed. To get the new models, use these');
  console.log('commands ON THE MACHINE AT 192.168.0.94:\n');
  
  console.log('  Option 1: Via Ollama CLI (if SSH available):');
  console.log('  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');
  console.log('  ssh user@192.168.0.94');
  console.log('  ollama pull mistral:7b-instruct-q4_K_M');
  console.log('  ollama pull llama2-uncensored:7b-q4_K_M');
  console.log('  ollama pull functionary:7b-v1\n');

  console.log('  Option 2: Directly on 192.168.0.94 machine:');
  console.log('  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');
  console.log('  ollama pull mistral:7b-instruct-q4_K_M');
  console.log('  ollama pull llama2-uncensored:7b-q4_K_M');
  console.log('  ollama pull functionary:7b-v1\n');

  console.log('  Each model will take 5-15 minutes depending on connection.\n');

  console.log('â•'.repeat(66));
  console.log('âœ“ READY TO TEST:\n');
  
  console.log('Once models are installed, test in Continue:\n');
  
  console.log('1. Open Continue in VS Code');
  console.log('2. Switch to different models using model selector');
  console.log('3. Try this prompt:\n');
  
  console.log('   â”Œâ”€ START PROMPT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”');
  console.log('   â”‚ Test my MCP tools:                                 â”‚');
  console.log('   â”‚ 1. Run: echo "MCP Works"                          â”‚');
  console.log('   â”‚ 2. List: C:\\Users\\imme\\CascadeProjects           â”‚');
  console.log('   â”‚ 3. Check git status                               â”‚');
  console.log('   â”‚ 4. Run: npm run test                              â”‚');
  console.log('   â”‚ Show results for each.                            â”‚');
  console.log('   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n');

  console.log('â•'.repeat(66));
  console.log('ğŸ“š TESTING GUIDE:\n');
  
  console.log('For detailed testing instructions, see:');
  console.log('  C:\\Users\\imme\\CascadeProjects\\CONTINUE-MCP-TESTING-GUIDE.md\n');
  
  console.log('Quick tests to verify everything works:');
  console.log('  1. Terminal command execution');
  console.log('  2. File read/write operations');
  console.log('  3. Git operations (status, log, diff)');
  console.log('  4. Python code execution');
  console.log('  5. Environment variables');
  console.log('  6. Multi-step task execution\n');

  console.log('â•'.repeat(66));
  console.log('âš™ï¸  CONTINUE CONFIGURATION:\n');
  
  console.log('Your Continue is configured with:');
  console.log(`  Config file: ${continueConfig}`);
  console.log(`  MCP server: node mcp-ollama-server.js`);
  console.log(`  Ollama base: http://192.168.0.94:11434\n`);
  
  console.log('Models configured (will appear once pulled):');
  console.log('  â€¢ Llama 3.1 8B (PRIMARY)');
  console.log('  â€¢ Mistral 7B (Fast & Smart)');
  console.log('  â€¢ Llama 2 Uncensored 7B (Reasoning)');
  console.log('  â€¢ Functionary 7B (Tool-Calling)\n');

  console.log('â•'.repeat(66));
  console.log('ğŸš€ SUMMARY:\n');
  
  console.log('âœ… What\'s Done:');
  console.log('  â€¢ MCP tools configured (30 tools available)');
  console.log('  â€¢ Continue setup complete');
  console.log('  â€¢ Ollama connection working');
  console.log('  â€¢ Test files and guides created\n');
  
  console.log('â³ What\'s Next:');
  console.log('  1. Manually pull 3 new models (they failed via API)');
  console.log('  2. Wait for models to download (~15-45 min total)');
  console.log('  3. Switch models in Continue to test each');
  console.log('  4. Run MCP tool tests in Continue chat\n');
  
  console.log('âœ“ After models are ready, Continue will work perfectly!');
  console.log('  You\'ll be able to use all 30 MCP tools with local LLMs.\n');

  console.log('â•'.repeat(66) + '\n');
}

main().catch(console.error);
